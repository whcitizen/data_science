{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Implment 'Multiply' layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AddLayer(object):\n",
    "    def forward(self,x,y):\n",
    "        z = x+y\n",
    "        return z\n",
    "    def backward(self,dz): #dz represents dL/dz propagated from the above layer\n",
    "        dx = dz #dL/dz * dz/dx\n",
    "        dy = dz #dL/dz * dz/dy\n",
    "        return [dx,dy]\n",
    "\n",
    "class MultiplyLayer(object):\n",
    "    def forward(self,x,y):\n",
    "        z = x*y\n",
    "        self.x = x  #keep activation during feed forwarding!\n",
    "        self.y = y\n",
    "        return z\n",
    "    def backward(self,dz): #dz represents dL/dz propgated from the above layer\n",
    "        ###TODO\n",
    "        #dx = ... #dL/dz * dz/dx\n",
    "        #dy = ... #dL/dz * dz/dy\n",
    "        return [dx,dy]\n",
    "    def numeric_grad(self,x,y,dz=1.0,eps=0.0001):\n",
    "        ###TODO\n",
    "        #dx = ...\n",
    "        #dy = ...\n",
    "        return [dx,dy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed forward operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add = AddLayer()\n",
    "mult = MultiplyLayer()\n",
    "add.forward(mult.forward(2,3),-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how backprop works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(add.backward(1))\n",
    "print(mult.backward(add.backward(1)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare analytical grads with numerical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mult.backward(dz=1))\n",
    "print(mult.numeric_grad(2,3,dz=1,eps=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: implement logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the digit dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img2vector(filename):\n",
    "    returnVect = np.zeros((1,1024)) #images are 32x32, constituting 1024-dim vectors\n",
    "    fr = open(filename)\n",
    "    for i in range(32):\n",
    "        lineStr = fr.readline()\n",
    "        for j in range(32):\n",
    "            returnVect[0,32*i+j] = int(lineStr[j])\n",
    "    return returnVect\n",
    "\n",
    "def loadDigits(dataDir):\n",
    "    labels = []\n",
    "    fileList = os.listdir(dataDir)\n",
    "    m = len(fileList)\n",
    "    dataMat = np.zeros((m,1024))\n",
    "    labelMat = np.zeros((m,10))\n",
    "    for i in range(m):\n",
    "        fileNameStr = fileList[i]  #load the training set\n",
    "        fileStr = fileNameStr.split('.')[0]  #take off \".txt\"\n",
    "        classNumStr = int(fileStr.split('_')[0])\n",
    "        labelMat[i,classNumStr]=1.0\n",
    "        dataMat[i,:] = img2vector('%s/%s' % (dataDir, fileNameStr))\n",
    "    return dataMat, labelMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainMat, trainLabels = loadDigits('trainingDigits') \n",
    "testMat, testLabels = loadDigits('testDigits') \n",
    "meanVec = np.average(trainMat,axis=0)\n",
    "stdVec = np.std(trainMat,axis=0)+1.0\n",
    "trainMat = (trainMat-meanVec)/stdVec\n",
    "testMat = (testMat-meanVec)/stdVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: implement Linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(object):\n",
    "    def __init__(self,in_size,out_size):\n",
    "        self.W = np.random.uniform(size=(in_size,out_size))  #random initialization\n",
    "        self.b = np.random.uniform(size=(out_size))\n",
    "        self.X = None\n",
    "        self.dX = None\n",
    "        self.db = None\n",
    "    def forward(self,X):\n",
    "        ###TODO\n",
    "        #return ...\n",
    "    def backward(self,dY):\n",
    "        ###TODO\n",
    "        #dX = ...\n",
    "        #self.dW = ...\n",
    "        #self.db = ...\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxCrossEnt(object):\n",
    "    def __init__(self):\n",
    "        self.Y = None\n",
    "        self.t = None\n",
    "    def forward(self,X,t):\n",
    "        expX = np.exp(X)\n",
    "        Y = expX / np.sum(expX, axis=1).reshape(-1,1)  #softmax\n",
    "        L = -np.sum(t*np.log(Y),axis=1)\n",
    "        self.Y,self.t = Y,t \n",
    "        return L\n",
    "    def backward(self,dZ=None):\n",
    "        return self.Y - self.t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train with an SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nsample = trainMat.shape[0]  #sample size\n",
    "bsize=30 #batch size\n",
    "niter = int(np.ceil(nsample/bsize)) #iteration per epoch\n",
    "max_epoch=10\n",
    "alpha = 0.0001 #learning rate\n",
    "\n",
    "L1 = Linear(1024,10)\n",
    "Loss = SoftmaxCrossEnt()\n",
    "\n",
    "for e in range(max_epoch):\n",
    "    rind = np.random.permutation(nsample) #shuffle data (this is important!)\n",
    "    trainMat = trainMat[rind]\n",
    "    trainLabels = trainLabels[rind]\n",
    "    for i in range(niter):\n",
    "        X = trainMat[i*bsize:(i+1)*bsize,:]\n",
    "        t = trainLabels[i*bsize:(i+1)*bsize]        \n",
    "        L = np.average(Loss.forward(L1.forward(X),t),axis=0)\n",
    "        \n",
    "        L1.backward(Loss.backward())\n",
    "        L1.W = L1.W - alpha*L1.dW\n",
    "        L1.b = L1.b - alpha*L1.db      \n",
    "        print(\"Epoch %d, Iteration %d: Loss %f\" % (e+1, i+1, L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: implement 3-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: implement sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###TODO\n",
    "class Sigmoid(object):\n",
    "    def forward(self,X):\n",
    "        return\n",
    "    def backward(self,dY):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: implement the network and solver (you can copy the one above and modify it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
